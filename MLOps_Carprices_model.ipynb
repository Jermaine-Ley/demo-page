{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "# matplotlib und seaborn für Grafiken\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sklearn für Überwachtes Lernen\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import max_error\n",
    "# keras für Neuronale Netze\n",
    "import keras as keras\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, LSTM, Dropout \n",
    "#from keras.utils import plot_model \n",
    "# Für das Darstellen von Bildern im SVG-Format\n",
    "#import graphviz as gv\n",
    "import pydot\n",
    "#from keras.utils import model_to_dot\n",
    "from IPython.display import SVG\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carP_df = pd.read_csv( 'car_prices.csv',nrows=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carP_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carP_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fehlende Werte erkennen. Gibt ein boolesches Objekt zurück, das angibt, ob die Werte NA sind \n",
    "carP_df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ab diesen Abschnitt werden die fehlenden Werte je nach Datentyp der Spalte. \n",
    "# \\ Kategorische Spalten -> Modus \\ Kontinuierliche Spalten -> Mittelwert/Median \\ Diskrete Spalten -> Modus\n",
    "\n",
    "\n",
    "categorical_columns = []\n",
    "continous_columns = []\n",
    "discrete_columns = []\n",
    "\n",
    "for x in carP_df.columns:\n",
    "  if carP_df[x].dtypes == 'O':\n",
    "    categorical_columns.append(x)\n",
    "  else:\n",
    "    if carP_df[x].nunique()>20:\n",
    "      continous_columns.append(x)\n",
    "    else:\n",
    "      discrete_columns.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Füllen fehlender Werte kategorischer Spalten mit Modus\n",
    "for x in categorical_columns:\n",
    "  carP_df[x].fillna(carP_df[x].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = 'condition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary with int64 featureumns as keys and np.int32 as values\n",
    "int_32 = dict.fromkeys(carP_df.select_dtypes(np.int64).columns, np.int32)\n",
    "# Change all columns from dictionary\n",
    "carP_df = carP_df.astype(int_32)\n",
    "\n",
    "# Make a dictionary with float64 columns as keys and np.float32 as values\n",
    "float_32 = dict.fromkeys(carP_df.select_dtypes(np.float64).columns, np.float32)\n",
    "carP_df = carP_df.astype(float_32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In String umwandeln\n",
    "\n",
    "# Eine Liste aller kategorischen Variablen erstellen\n",
    "cat_convert = ['make', 'model', 'trim', 'body', 'transmission', 'vin', 'state', 'color', 'interior', 'seller', 'saledate']\n",
    "\n",
    "# convert variables\n",
    "for i in cat_convert:\n",
    "    carP_df[i] = carP_df[i].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In kategorische Variablen umwandeln\n",
    "\n",
    "# Eine Liste aller kategorischen Variablen erstellen\n",
    "cat_convert = ['year', 'mmr', 'sellingprice']\n",
    "\n",
    "# convert variables\n",
    "for i in cat_convert:\n",
    "    carP_df[i] = carP_df[i].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carP_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste aller numerischen Daten erstellen (außer Bezeichnung)\n",
    "list_num = carP_df.drop(columns=[y_label]).select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Erstellen Sie eine Liste aller kategorialen Daten, die als Ganzzahlen gespeichert sind (außer Label).\n",
    "list_cat_int = carP_df.drop(columns=[y_label]).select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "# Liste aller kategorischen Daten erstellen, die als String gespeichert sind (außer Label)\n",
    "list_cat_string = carP_df .drop(columns=[y_label]).select_dtypes(include=['string']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validierungsdaten erstellen\n",
    "df_val = carP_df.sample(frac=0.2, random_state=1337)\n",
    "\n",
    "# Trainingsdaten erstellen\n",
    "df_train = carP_df.drop(df_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Using %d samples for training and %d for validation\"\n",
    "    % (len(df_train), len(df_val))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to Tensors¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create our tensors\n",
    "\n",
    "def dataframe_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    df = dataframe.copy()\n",
    "    labels = df.pop(y_label)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    df = ds.prefetch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "ds_train = dataframe_to_dataset(df_train, shuffle=True, batch_size=batch_size)\n",
    "ds_val = dataframe_to_dataset(df_val, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical preprocessing function\n",
    "def get_normalization_layer(name, dataset):\n",
    "    \n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "    # Prepare a dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    return normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical preprocessing functions¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  \n",
    "  # Create a layer that turns strings into integer indices.\n",
    "  if dtype == 'string':\n",
    "    index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  # Otherwise, create a layer that turns integer values into integer indices.\n",
    "  else:\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Encode the integer indices.\n",
    "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "  return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  \n",
    "  # Create a layer that turns strings into integer indices.\n",
    "  if dtype == 'string':\n",
    "    index = layers.StringLookup(max_tokens=max_tokens)\n",
    "  # Otherwise, create a layer that turns integer values into integer indices.\n",
    "  else:\n",
    "    index = layers.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a `tf.data.Dataset` that only yields the feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Encode the integer indices.\n",
    "  encoder = layers.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply multi-hot encoding to the indices. The lambda function captures the\n",
    "  # layer, so you can use them, or include them in the Keras Functional model later.\n",
    "  return lambda feature: encoder(index(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "for feature in list_num:\n",
    "  numeric_feature = tf.keras.Input(shape=(1,), name=feature)\n",
    "  normalization_layer = get_normalization_layer(feature, ds_train)\n",
    "  encoded_numeric_feature = normalization_layer(numeric_feature)\n",
    "  all_inputs.append(numeric_feature)\n",
    "  encoded_features.append(encoded_numeric_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical preprocessing¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in list_cat_int:\n",
    "  categorical_feature = tf.keras.Input(shape=(1,), name=feature, dtype='int32')\n",
    "  encoding_layer = get_category_encoding_layer(name=feature,\n",
    "                                               dataset=ds_train,\n",
    "                                               dtype='int32',\n",
    "                                               max_tokens=5)\n",
    "  encoded_categorical_feature = encoding_layer(categorical_feature)\n",
    "  all_inputs.append(categorical_feature)\n",
    "  encoded_features.append(encoded_categorical_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in list_cat_string:\n",
    "  categorical_feature = tf.keras.Input(shape=(1,), name=feature, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(name=feature,\n",
    "                                               dataset=ds_train,\n",
    "                                               dtype='string',\n",
    "                                               max_tokens=5)\n",
    "  encoded_categorical_feature = encoding_layer(categorical_feature)\n",
    "  all_inputs.append(categorical_feature)\n",
    "  encoded_features.append(encoded_categorical_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "all_features = layers.concatenate(encoded_features)\n",
    "\n",
    "# First layer\n",
    "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
    "\n",
    "# Dropout to prevent overvitting\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output layer\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Group all layers \n",
    "model = tf.keras.Model(all_inputs, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model.compile konfiguriert das Modell für das Training:\n",
    "\n",
    "    Optimierer: Der Mechanismus, durch den sich das Modell auf der Grundlage der Trainingsdaten selbst aktualisiert, um seine Leistung zu verbessern. Eine gängige Option für den Optimierer ist Adam, eine stochastische Gradientenabstiegsmethode, die auf der adaptiven Schätzung von Momenten erster und zweiter Ordnung beruht.\n",
    "\n",
    "    Verlust: Die Art und Weise, wie das Modell seine Leistung anhand der Trainingsdaten messen kann und wie es sich somit selbst in die richtige Richtung lenken kann. Das bedeutet, dass der Zweck von Verlustfunktionen darin besteht, die Größe zu berechnen, die ein Modell während des Trainings zu minimieren versuchen sollte.\n",
    "\n",
    "    Metriken: Eine Metrik ist eine Funktion, die verwendet wird, um die Leistung Ihres Modells während des Trainings und der Tests zu beurteilen. Hier geht es nur um die Genauigkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", \n",
    "              loss =\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "# `rankdir='LR'` is to make the graph horizontal.\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 49s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 47s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 45s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 49s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 47s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 45s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 47s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 48s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 45s 4ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f930714eb0>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=10, validation_data=ds_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 7s 3ms/step - loss: nan - accuracy: 0.0000e+00A\n",
      "Accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(ds_val)\n",
    "\n",
    "print(\"Accuracy\", round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c59d19f00193f2faebd1965e339288c86326293cc9051b701718804fd37abb29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
